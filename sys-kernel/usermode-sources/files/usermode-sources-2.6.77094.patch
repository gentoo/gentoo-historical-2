diff -urp linux-2.6.8.1-r7/drivers/block/scsi_ioctl.c linux-2.6.8.1-r8/drivers/block/scsi_ioctl.c
--- linux-2.6.8.1-r7/drivers/block/scsi_ioctl.c	2004-08-14 11:56:23.000000000 +0100
+++ linux-2.6.8.1-r8/drivers/block/scsi_ioctl.c	2005-01-09 12:09:55.345308528 +0000
@@ -304,7 +304,8 @@ static int sg_scsi_ioctl(struct file *fi
 			 struct gendisk *bd_disk, Scsi_Ioctl_Command __user *sic)
 {
 	struct request *rq;
-	int err, in_len, out_len, bytes, opcode, cmdlen;
+	unsigned int in_len, out_len, bytes, opcode, cmdlen;
+	int err;
 	char *buffer = NULL, sense[SCSI_SENSE_BUFFERSIZE];
 
 	/*
diff -urp linux-2.6.8.1-r7/drivers/char/moxa.c linux-2.6.8.1-r8/drivers/char/moxa.c
--- linux-2.6.8.1-r7/drivers/char/moxa.c	2005-01-09 12:06:21.000000000 +0000
+++ linux-2.6.8.1-r8/drivers/char/moxa.c	2005-01-09 12:09:55.327311264 +0000
@@ -1687,6 +1687,8 @@ int MoxaDriverIoctl(unsigned int cmd, un
 		return -EFAULT;
 	if(dltmp.cardno < 0 || dltmp.cardno >= MAX_BOARDS)
 		return -EINVAL;
+	if(dltmp.len < 0 || dltmp.len > sizeof(moxaBuff))
+		return -EINVAL;
 
 	switch(cmd)
 	{
@@ -2841,8 +2843,6 @@ static int moxaload320b(int cardno, unsi
 	unsigned long baseAddr;
 	int i;
 
-	if(len > sizeof(moxaBuff))
-		return -EINVAL;
 	if(copy_from_user(moxaBuff, tmp, len))
 		return -EFAULT;
 	baseAddr = moxaBaseAddr[cardno];
diff -urp linux-2.6.8.1-r7/drivers/char/random.c linux-2.6.8.1-r8/drivers/char/random.c
--- linux-2.6.8.1-r7/drivers/char/random.c	2004-08-14 11:54:48.000000000 +0100
+++ linux-2.6.8.1-r8/drivers/char/random.c	2005-01-09 12:09:55.358306552 +0000
@@ -1917,7 +1917,7 @@ static int poolsize_strategy(ctl_table *
 			     void __user *oldval, size_t __user *oldlenp,
 			     void __user *newval, size_t newlen, void **context)
 {
-	int	len;
+	size_t	len;
 	
 	sysctl_poolsize = random_state->poolinfo.POOLBYTES;
 
diff -urp linux-2.6.8.1-r7/include/linux/writeback.h linux-2.6.8.1-r8/include/linux/writeback.h
--- linux-2.6.8.1-r7/include/linux/writeback.h	2004-08-14 11:54:49.000000000 +0100
+++ linux-2.6.8.1-r8/include/linux/writeback.h	2005-01-09 12:09:55.000000000 +0000
@@ -74,6 +74,7 @@ static inline void wait_on_inode(struct 
 int wakeup_bdflush(long nr_pages);
 void laptop_io_completion(void);
 void laptop_sync_completion(void);
+void throttle_vm_writeout(void);
 
 /* These are exported to sysctl. */
 extern int dirty_background_ratio;
diff -urp linux-2.6.8.1-r7/mm/mmap.c linux-2.6.8.1-r8/mm/mmap.c
--- linux-2.6.8.1-r7/mm/mmap.c	2005-01-09 12:06:23.000000000 +0000
+++ linux-2.6.8.1-r8/mm/mmap.c	2005-01-09 12:09:55.000000000 +0000
@@ -1223,6 +1223,13 @@ int expand_stack(struct vm_area_struct *
 		vm_unacct_memory(grow);
 		return -ENOMEM;
 	}
+	if ((vma->vm_flags & VM_LOCKED) && !capable(CAP_IPC_LOCK) &&
+			((vma->vm_mm->locked_vm + grow) << PAGE_SHIFT) >
+			current->rlim[RLIMIT_MEMLOCK].rlim_cur) {
+		anon_vma_unlock(vma);
+		vm_unacct_memory(grow);
+		return -ENOMEM;
+	}
 	vma->vm_end = address;
 	vma->vm_mm->total_vm += grow;
 	if (vma->vm_flags & VM_LOCKED)
@@ -1284,6 +1291,13 @@ int expand_stack(struct vm_area_struct *
 		vm_unacct_memory(grow);
 		return -ENOMEM;
 	}
+	if ((vma->vm_flags & VM_LOCKED) && !capable(CAP_IPC_LOCK) &&
+			((vma->vm_mm->locked_vm + grow) << PAGE_SHIFT) >
+			current->rlim[RLIMIT_MEMLOCK].rlim_cur) {
+		anon_vma_unlock(vma);
+		vm_unacct_memory(grow);
+		return -ENOMEM;
+	}
 	vma->vm_start = address;
 	vma->vm_pgoff -= grow;
 	vma->vm_mm->total_vm += grow;
diff -urp linux-2.6.8.1-r7/mm/page-writeback.c linux-2.6.8.1-r8/mm/page-writeback.c
--- linux-2.6.8.1-r7/mm/page-writeback.c	2004-08-14 11:55:47.000000000 +0100
+++ linux-2.6.8.1-r8/mm/page-writeback.c	2005-01-09 12:09:55.000000000 +0000
@@ -276,6 +276,28 @@ void balance_dirty_pages_ratelimited(str
 }
 EXPORT_SYMBOL(balance_dirty_pages_ratelimited);
 
+void throttle_vm_writeout(void)
+{
+	struct writeback_state wbs;
+	long background_thresh;
+	long dirty_thresh;
+
+        for ( ; ; ) {
+		get_dirty_limits(&wbs, &background_thresh, &dirty_thresh);
+
+                /*
+                 * Boost the allowable dirty threshold a bit for page
+                 * allocators so they don't get DoS'ed by heavy writers
+                 */
+                dirty_thresh += dirty_thresh / 10;      /* wheeee... */
+
+                if (wbs.nr_unstable + wbs.nr_writeback <= dirty_thresh)
+                        break;
+                blk_congestion_wait(WRITE, HZ/10);
+        }
+}
+
+
 /*
  * writeback at least _min_pages, and keep writing until the amount of dirty
  * memory is less than the background threshold, or until we're all clean.
diff -urp linux-2.6.8.1-r7/mm/vmscan.c linux-2.6.8.1-r8/mm/vmscan.c
--- linux-2.6.8.1-r7/mm/vmscan.c	2004-08-14 11:54:50.000000000 +0100
+++ linux-2.6.8.1-r8/mm/vmscan.c	2005-01-09 12:10:52.000000000 +0000
@@ -362,9 +362,6 @@ static int shrink_list(struct list_head 
 
 		BUG_ON(PageActive(page));
 
-		if (PageWriteback(page))
-			goto keep_locked;
-
 		sc->nr_scanned++;
 		/* Double the slab pressure for mapped and swapcache pages */
 		if (page_mapped(page) || PageSwapCache(page))
@@ -841,6 +838,8 @@ shrink_zone(struct zone *zone, struct sc
 				break;
 		}
 	}
+
+	throttle_vm_writeout();
 }
 
 /*
